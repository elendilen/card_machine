
from fastapi import FastAPI, File, UploadFile,Response,WebSocket,Request
from fastapi.responses import JSONResponse,FileResponse,StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
import torchaudio
import whisper
import tempfile
import os
import cv2
import threading,time
import subprocess
import uuid
from gesture import GestureRecognizer
import tensorflow as tf
import librosa
import numpy as np
import torch
import pickle
from insightface.app import FaceAnalysis
import requests
import base64
from datetime import datetime
from flask import request, jsonify



# æ·»åŠ  ffmpegæ‰€åœ¨è·¯å¾„åˆ°ç¯å¢ƒå˜é‡
ffmpeg_path = r"C:\ffmpeg-7.1.1-essentials_build\ffmpeg-7.1.1-essentials_build\bin"
os.environ["PATH"] += os.pathsep + ffmpeg_path


app = FastAPI()

app.mount("/static", StaticFiles(directory="../front/static"), name="static")

torchaudio.set_audio_backend("soundfile")
@app.get("/")
async def index():
    return FileResponse("../front/static/index2.html")

# å…è®¸è·¨åŸŸ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ–æ¨¡å‹
whisper_model = whisper.load_model("medium")
gesture_recognizer = GestureRecognizer()
cap = cv2.VideoCapture(0)

MODEL_PATH="speaker_cnn4.h5"
model = tf.keras.models.load_model(MODEL_PATH)

label_map = {0: "SHEN", 1: "TIAN", 2: "WAN", 3: "WANG"}

SR = 16000
N_MELS = 64
MAX_LEN = 320

# ===== æå– Mel ç‰¹å¾å‡½æ•° =====
def extract_mel(y, sr=16000, n_mels=64, max_len=320):
    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)
    log_mel = librosa.power_to_db(mel_spec, ref=np.max)
    # Padding
    if log_mel.shape[1] > max_len:
        log_mel = log_mel[:, :max_len]
    else:
        pad_width = max_len - log_mel.shape[1]
        log_mel = np.pad(log_mel, ((0, 0), (0, pad_width)), mode='constant')
    return log_mel

camera_active = False
lock = threading.Lock()

def generate_frames():
    cap = cv2.VideoCapture(0)
    while True:
        with lock:
            if not camera_active:
                break
        ret, frame = cap.read()

        if not ret:
            break
        frame = cv2.flip(frame, 0)  # ä¸Šä¸‹é¢ å€’
        _, buffer = cv2.imencode('.jpg', frame)
        yield (b"--frame\r\n"
               b"Content-Type: image/jpeg\r\n\r\n" + buffer.tobytes() + b"\r\n")
    cap.release()

@app.post("/api/start_stream")
def start_stream():
    global camera_active
    with lock:
        camera_active = True
    return {"status": "started"}

@app.get("/api/video_feed")
def video_feed():
    return Response(cap = cv2.VideoCapture(0), media_type="multipart/x-mixed-replace; boundary=frame")

@app.post("/api/stop_stream")
def stop_stream():
    global camera_active
    with lock:
        camera_active = False
    return {"status": "stopped"}



@app.websocket("/ws/gesture")
async def gesture_ws(websocket: WebSocket):
    await websocket.accept()
    while True:
        ret, frame = cap.read()
        if not ret:
            continue

        # æ—‹è½¬ 180 åº¦ï¼ˆå¦‚æœæ‘„åƒå¤´å€’è£…ï¼‰
        frame = cv2.rotate(frame, cv2.ROTATE_180)

        # é¢„æµ‹æ‰‹åŠ¿
        prediction = gesture_recognizer.predict(frame)

        if prediction:
            await websocket.send_json({"prediction": prediction})


dialogue_history = []
"""
@app.post("/api/recognize/")
async def recognize_audio(file: UploadFile = File(...)):
    try:
        # ä¸´æ—¶æ–‡ä»¶å¤„ç†
        with tempfile.NamedTemporaryFile(delete=False, suffix=".webm") as tmp_in:
            tmp_in.write(await file.read())
            tmp_in_path = tmp_in.name

        tmp_out_path = os.path.join(tempfile.gettempdir(), f"{uuid.uuid4().hex}.wav")

        # ffmpeg è½¬ç 
        subprocess.run([
            "ffmpeg", "-y", "-i", tmp_in_path, "-ar", str(SR), "-ac", "1", tmp_out_path
        ], check=True)

        # Whisper è¯­éŸ³è½¬æ–‡å­—
        result = whisper_model.transcribe(tmp_out_path, language="en", task="transcribe")
        text = result.get("text", "").strip()

        # CNN å£°çº¹è¯†åˆ«
        y, _ = librosa.load(tmp_out_path, sr=SR, mono=True, dtype=np.float32)
        if np.max(np.abs(y)) > 0:
            y = y / np.max(np.abs(y))  # å½’ä¸€åŒ–
        y, _ = librosa.effects.trim(y)
        feature = extract_mel(y, sr=SR, n_mels=N_MELS, max_len=MAX_LEN)
        feature = feature[np.newaxis, ..., np.newaxis]  # (1, 64, 320, 1)

        pred = model.predict(feature)
        speaker_id = np.argmax(pred)
        confidence = float(pred[0][speaker_id])
        speaker_name = label_map.get(speaker_id, "Unknown")

        dialogue_history.append({
            "player": speaker_name,
            "text": text
        })
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(tmp_in_path)
        os.remove(tmp_out_path)

        # è¿”å›å‰ç«¯éœ€è¦çš„ç»“æ„
        return {
            "player": speaker_name,
            "text": text,  #ä¿ç•™æ–‡æœ¬
            "confidence": confidence
        }

    except Exception as e:
        import traceback
        traceback.print_exc()
        return JSONResponse(status_code=500, content={"error": str(e)})
"""

from ecapa_identifier import ECAPAIdentifier  # æ”¾åœ¨ app.py é¡¶éƒ¨å¯¼å…¥
identifier = ECAPAIdentifier()               # æ”¾åœ¨å…¨å±€åˆå§‹åŒ–åŒºåŸŸ

@app.post("/api/recognize/")
async def recognize_audio(file: UploadFile = File(...)):
    try:
        # ä¸´æ—¶æ–‡ä»¶å¤„ç†
        with tempfile.NamedTemporaryFile(delete=False, suffix=".webm") as tmp_in:
            tmp_in.write(await file.read())
            tmp_in_path = tmp_in.name

        tmp_out_path = os.path.join(tempfile.gettempdir(), f"{uuid.uuid4().hex}.wav")

        # ffmpeg è½¬ç ä¸º wav
        subprocess.run([
            "ffmpeg", "-y", "-i", tmp_in_path, "-ar", str(SR), "-ac", "1", tmp_out_path
        ], check=True)

        # Whisper è¯­éŸ³è½¬æ–‡å­—ï¼ˆè¯†åˆ«å†…å®¹ï¼‰
        result = whisper_model.transcribe(tmp_out_path, language="en", task="transcribe")
        text = result.get("text", "").strip()

        # âœ… æ›¿æ¢ä¸º ECAPA å£°çº¹è¯†åˆ«
        speaker_name, confidence = identifier.identify(tmp_out_path)

        # è®°å½•åˆ°å¯¹è¯å†å²ä¸­
        dialogue_history.append({
            "player": speaker_name,
            "text": text
        })

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(tmp_in_path)
        os.remove(tmp_out_path)

        return {
            "player": speaker_name,
            "text": text,
            "confidence": float(confidence)
        }

    except Exception as e:
        import traceback
        traceback.print_exc()
        return JSONResponse(status_code=500, content={"error": str(e)})
 
    
from gameManager import GameManager
from pydantic import BaseModel

game=GameManager()

# æ•°æ®æ¨¡å‹
class InitRequest(BaseModel):
    num_players: int

class GestureInput(BaseModel):
    gesture: str

class RoleUpdateInput(BaseModel):
    player_id: int
    
# åˆå§‹åŒ–æ¸¸æˆ
@app.post("/init_game")
def init_game(data: InitRequest):
    game.init_players(data.num_players)
    return {"message": f"åˆå§‹åŒ– {data.num_players} åç©å®¶æˆåŠŸ", "state": game.state}

# æ›´æ–°è§’è‰²ï¼ˆé€šè¿‡äººè„¸è¯†åˆ«ï¼‰
@app.post("/update_role")
def update_role(data: RoleUpdateInput):
    success = game.update_role(data.player_id)
    if success:
        return {"message": f"ç©å®¶ {data.player_id} è¢«æ ‡è®°ä¸º {game.state['current_role']}"}
    return {"message": "å½“å‰é˜¶æ®µæ— æ³•åˆ†é…è§’è‰²"}

# è·å–å½“å‰çŠ¶æ€
@app.get("/get_game_state")
def get_game_state():
    return game.state

#æäº¤æ‰‹åŠ¿
class GestureInput(BaseModel):
    gesture: str

@app.post("/submit_gesture")
async def submit_gesture(data: GestureInput):
    gesture = data.gesture
    if gesture == "None":
        print("ğŸ•³ï¸ æ‰‹åŠ¿ä¸ºç©ºï¼Œè·³è¿‡è¯¥ç©å®¶æ“ä½œ")
        return {"message": "æ— æ‰‹åŠ¿è¾“å…¥ï¼Œè·³è¿‡æ“ä½œ", "skip": True}
    print(f"æ”¶åˆ°æ‰‹åŠ¿: {gesture}")
    
    game.handle_action(gesture)  # handle_action é‡Œå·²ç» next_role äº†
    
    print("å½“å‰ç©å®¶çŠ¶æ€ï¼š")
    for pid, info in game.state["players"].items():
        print(f"ç©å®¶ {pid} - è§’è‰²: {info['role']}, å­˜æ´»: {info['alive']}")

    return {
        "message": f"æ‰‹åŠ¿ {gesture} å·²å¤„ç†",
        "current_role": game.state["current_role"],
        "players": game.players
    }



from pydantic import BaseModel
from typing import Optional

class VoteResultInput(BaseModel):
    eliminated: Optional[int]


@app.post("/submit_vote")
def submit_vote(data: VoteResultInput):
    print("ğŸŸ¡ æ”¶åˆ°æ¥è‡ª 5000 çš„æŠ•ç¥¨ç»“æœè¯·æ±‚")
    print("ğŸ“© è¯·æ±‚ä½“å†…å®¹ï¼š", data)

    eliminated = data.eliminated

    if eliminated is not None:
        print(f"ğŸ”§ å°è¯•æ”¾é€ç©å®¶ {eliminated}")
        success, msg = game.eliminate_player(eliminated)
        # æ›´æ–°å½“å‰æç¤ºä¿¡æ¯ï¼Œæ–¹ä¾¿å‰ç«¯åˆ·æ–°
        game.state["current_prompt"] = f"æŠ•ç¥¨ç»“æœï¼š{eliminated}å·è¢«æ”¾é€"

        print(f"ç©å®¶ {eliminated} è¢«æˆåŠŸæ”¾é€") if success else print("âŒ æ”¾é€å¤±è´¥")
    else:
        # âœ… å¹³ç¥¨æƒ…å†µï¼šä¸æ”¾é€ä»»ä½•äºº
        game.state["eliminated_today"] = None
        game.state["current_prompt"] = "å¹³ç¥¨ï¼Œæ— äººè¢«æ”¾é€"
        success = True
        msg = "å¹³ç¥¨ï¼Œæ— äººè¢«å¤„å†³"
        print("âš–ï¸ å¹³ç¥¨ï¼Œæ— äººè¢«æ”¾é€")

    print("ğŸ“Š ç©å®¶ç”Ÿå­˜çŠ¶æ€æ›´æ–°ï¼š")
    for pid, info in game.state["players"].items():
        print(f"ç©å®¶ {pid} - å­˜æ´»: {info['alive']}")

    # âœ… ç»Ÿä¸€æ¨è¿›é€»è¾‘ï¼ˆä¸ç®¡æœ‰æ²¡æœ‰æ”¾é€æˆåŠŸï¼‰
    game.check_game_end()
    
    game.advance_phase()
    print("å½“å‰é˜¶æ®µï¼š", game.state["phase"])


    return {"success": success, "message": msg, "eliminated": eliminated}

    
def build_prompt(dialogue: list[dict]) -> str:
    prompt = "ã€ç‹¼äººæ€å‘è¨€è®°å½•ã€‘\n"
    for idx, d in enumerate(dialogue, 1):
        prompt += f"ç©å®¶{idx}ï¼ˆ{d['player']}ï¼‰ï¼š{d['text']}\n"
    prompt += "\nè¯·æ ¹æ®ä¸Šé¢çš„å‘è¨€ï¼Œåˆ†ææ¯ä½ç©å®¶çš„å¯èƒ½èº«ä»½ï¼Œå¹¶è¯´æ˜ç†ç”±ã€‚æœ€åéœ€è¦å°†å›ç­”è½¬æ¢æˆè‹±æ–‡ã€‚"
    return prompt

@app.post("/api/deduce/")
async def llm_deduction():
    api_key = "sk-866c8c007f1a4440a2158962c6f28771"  # âœ… æ­£å¼éƒ¨ç½²å»ºè®®æ”¹ä¸ºç¯å¢ƒå˜é‡
    url = "https://api.deepseek.com/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }

    prompt = build_prompt(dialogue_history)

    payload = {
        "model": "deepseek-chat",
        "messages": [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªç‹¼äººæ€è£åˆ¤ï¼Œæ ¹æ®å‘è¨€å†…å®¹æ¨ç†èº«ä»½ï¼Œå¹¶ä¸”éœ€è¦ç»™å‡ºç›¸åº”çš„ç†ç”±ï¼Œè¯·å°†å›ç­”è½¬æ¢æˆè‹±æ–‡ã€‚"
            },
            {
                "role": "user",
                "content": prompt
            }
        ],
        "stream": False
    }

    try:
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()
        data = response.json()
        return {
            "prompt": prompt,
            "analysis": data["choices"][0]["message"]["content"]
        }
    except Exception as e:
        return {"error": str(e)}

@app.post("/advance_phase")
def advance_phase():
    game.advance_phase()
    return {
        "message": "é˜¶æ®µå·²æ¨è¿›",
        "current_phase": game.state["phase"],
        "current_role": game.state["current_role"],
        "round": game.state["round"]
    }




